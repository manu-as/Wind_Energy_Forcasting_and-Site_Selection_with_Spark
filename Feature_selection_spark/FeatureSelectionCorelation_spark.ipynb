{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Collecting pyspark\n",
      "  Using cached pyspark-3.5.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\python311\\lib\\site-packages (from pyspark) (0.10.9.7)\n",
      "Installing collected packages: pyspark\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\python311\\lib\\site-packages\\vboxapi-1.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "ERROR: Could not install packages due to an OSError: [Errno 13] Permission denied: 'c:\\\\Python311\\\\Scripts\\\\beeline'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features:\n",
      "Index(['windspeed_100m', 'windspeed_10m', 'windgusts_10m', 'dewpoint_2m',\n",
      "       'temperature_2m', 'winddirection_100m', 'winddirection_10m',\n",
      "       'relativehumidity_2m', 'Quarter', 'Month', 'Year'],\n",
      "      dtype='object')\n",
      "Selected Features:\n",
      "Index(['windspeed_100m', 'windspeed_10m', 'windgusts_10m', 'dewpoint_2m',\n",
      "       'temperature_2m', 'winddirection_100m', 'winddirection_10m',\n",
      "       'relativehumidity_2m', 'Quarter', 'Month', 'Year'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "import os\n",
    "\n",
    "# Step 4: Initialize Spark Session\n",
    "os.environ['SPARK_HOME'] = 'C:/spark-3.5.0-bin-hadoop3'\n",
    "os.environ['PATH'] += 'C:/spark-3.5.0-bin-hadoop3/bin'\n",
    "\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MLExample\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Step 5: Load Data into Spark DataFrame\n",
    "df = spark.read.csv('../Dataset/Location1_preprocessed.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Step 6: Data Preprocessing\n",
    "# Drop columns with missing values\n",
    "df_no_null = df.dropna()\n",
    "\n",
    "# Drop irrelevant columns\n",
    "df_no_date = df_no_null.drop('Time', 'Day')\n",
    "\n",
    "# Step 7: Feature Scaling\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "# Step 8: Correlation Analysis\n",
    "# Convert DataFrame to Pandas for correlation matrix calculation (Optional)\n",
    "df_pandas = df_no_date.toPandas()\n",
    "correlation_matrix = df_pandas.corr()\n",
    "\n",
    "# Select the features with the highest absolute correlation with the target variable\n",
    "target_variable = 'Power'\n",
    "selected_features = correlation_matrix.loc[correlation_matrix.index != target_variable, target_variable].abs().sort_values(ascending=False).index\n",
    "print(\"Selected Features:\")\n",
    "print(selected_features)\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "# Step 4: Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MLExample\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Step 5: Load Data into Spark DataFrame\n",
    "df = spark.read.csv('../Dataset/Location1_preprocessed.csv', header=True, inferSchema=True)\n",
    "\n",
    "\n",
    "# Step 6: Data Preprocessing\n",
    "# Drop columns with missing values\n",
    "df_no_null = df.dropna()\n",
    "\n",
    "# Drop irrelevant columns\n",
    "df_no_date = df_no_null.drop('Time', 'Day')\n",
    "\n",
    "# Step 7: Feature Scaling\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "# Step 8: Correlation Analysis\n",
    "# Convert DataFrame to Pandas for correlation matrix calculation (Optional)\n",
    "df_pandas = df_no_date.toPandas()\n",
    "correlation_matrix = df_pandas.corr()\n",
    "\n",
    "# Select the features with the highest absolute correlation with the target variable\n",
    "target_variable = 'Power'\n",
    "selected_features = correlation_matrix.loc[correlation_matrix.index != target_variable, target_variable].abs().sort_values(ascending=False).index\n",
    "print(\"Selected Features:\")\n",
    "print(selected_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
