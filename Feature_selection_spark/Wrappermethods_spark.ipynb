{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler, MinMaxScaler, RFormula\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "import findspark\n",
    "\n",
    "# Specify the Spark home directory and version\n",
    "findspark.init('C:/spark/spark-3.5.1-bin-hadoop3/spark-3.5.1-bin-hadoop3')\n",
    "\n",
    "# Configure Spark to use a master URL and set up the application name\n",
    "master_url = \"spark://192.168.57.215:7077\"\n",
    "app_name = \"Wrapper-Method\"\n",
    "\n",
    "# Create a SparkSession with the specified master and app name\n",
    "spark = SparkSession.builder.master(master_url).appName(app_name).getOrCreate()\n",
    "\n",
    "# Step 2: Load Data into Spark DataFrame\n",
    "data_path = \"../Dataset/Location1_preprocessed.csv\"  # Replace with your data path\n",
    "df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Step 3: Separate the time column (if applicable)\n",
    "time_columns = [\"Time\", \"Day\"]  # Adjust column names if needed\n",
    "if any(col in df.columns for col in time_columns):\n",
    "    time_column = df.select(*time_columns)\n",
    "    df_without_time = df.drop(*time_columns)\n",
    "else:\n",
    "    time_column = None\n",
    "    df_without_time = df\n",
    "\n",
    "# Step 4: Use MinMaxScaler to normalize the columns (if necessary)\n",
    "if any(col.endswith(\"numeric\") for col in df_without_time.columns):  # Check for numeric columns\n",
    "    assembler = VectorAssembler(inputCols=df_without_time.columns, outputCol=\"features\")\n",
    "    df_vectorized = assembler.transform(df_without_time)\n",
    "\n",
    "    scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "    scaler_model = scaler.fit(df_vectorized)\n",
    "    df_normalized = scaler_model.transform(df_vectorized)\n",
    "else:\n",
    "    df_normalized = df_without_time\n",
    "\n",
    "# Step 5: Add the time column back (if applicable)\n",
    "if time_column is not None:\n",
    "    df_normalized = df_normalized.join(time_column, how='inner')\n",
    "\n",
    "# Step 6: Define features (X) and target variable (y)\n",
    "rformula = RFormula(formula=\"Power ~ .\", featuresCol=\"new-features\", labelCol=\"label\")\n",
    "output = rformula.fit(df_normalized).transform(df_normalized)\n",
    "\n",
    "# Fix typo in the original code (change \"new-features\" to \"features\")\n",
    "features_col = \"features\"\n",
    "labels_col = \"label\"\n",
    "X = output.select(features_col)\n",
    "y = output.select(labels_col)\n",
    "\n",
    "# Step 7: Split the data for training, validation, and testing\n",
    "train_data, valid_data, test_data = output.randomSplit([0.8, 0.1, 0.1], seed=42)\n",
    "\n",
    "# Step 8: Perform RFE (optional)\n",
    "use_rfe = True  # Set to False to disable RFE\n",
    "num_features = 3  # Number of features to select by RFE (if enabled)\n",
    "\n",
    "if use_rfe:\n",
    "    pandas_df = output.toPandas()\n",
    "    X_sklearn = pandas_df.drop(\"label\", axis=1)\n",
    "    y_sklearn = pandas_df[\"label\"]\n",
    "\n",
    "    sklearn_rfe = RFE(estimator=RandomForestRegressor(n_estimators=100, random_state=42), n_features_to_select=num_features)\n",
    "    sklearn_rfe.fit(X_sklearn, y_sklearn)\n",
    "\n",
    "    selected_features = X_sklearn.columns[sklearn_rfe.support_]\n",
    "    selected_features = [\"label\"] + list(selected_features)\n",
    "\n",
    "    selected_df = output.select(selected_features)\n",
    "else:\n",
    "    selected_df = output  # Use all features if RFE is not enabled\n",
    "\n",
    "# Step 9: Train the model\n",
    "model = RandomForestRegressor(numTrees=100, seed=42)\n",
    "model_fit = model.fit(train_data)\n",
    "\n",
    "# Step 10: Make predictions on the test set\n",
    "predictions = model_fit.transform(test_data)\n",
    "\n",
    "# Step 11: Evaluate the model performance\n",
    "evaluator = RegressionEvaluator(labelCol=labels_col, predictionCol=\"prediction\", metricName=\"mse\")\n",
    "mse = evaluator.evaluate(predictions)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Step 12: Get the selected features (if RFE was used)\n",
    "if use_rfe:\n",
    "    selected_feature_names = selected_df.columns[1:]\n",
    "    print(\"Selected Features:\")\n",
    "    for feature_name in selected_feature_names:\n",
    "        print(feature_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
