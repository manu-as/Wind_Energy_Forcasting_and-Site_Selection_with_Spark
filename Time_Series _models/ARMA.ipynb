{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lag\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "#from pyspark.ml.regression import ARMA\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SparkSession\n",
    "os.environ['SPARK_HOME'] = 'C:/spark-3.5.0-bin-hadoop3'\n",
    "os.environ['PATH'] += 'C:/spark-3.5.0-bin-hadoop3/bin'\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TimeSeriesAnalysis\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = spark.read.csv('../Dataset/T1.csv', header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant columns with double backticks to preserve case sensitivity\n",
    "dataset = df.select(df[\"Date/Time\"].alias(\"timeStamp\"), df[\"Wind Speed (m/s)\"].alias(\"windSpeed\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[timeStamp: string, windSpeed: double]\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+\n",
      "|timeStamp          |windSpeed  |\n",
      "+-------------------+-----------+\n",
      "|2018-01-01 00:00:00|5.31133604 |\n",
      "|2018-01-01 00:10:00|5.672166824|\n",
      "|2018-01-01 00:20:00|5.216036797|\n",
      "|2018-01-01 00:30:00|5.659674168|\n",
      "|2018-01-01 00:40:00|5.577940941|\n",
      "|2018-01-01 00:50:00|5.604052067|\n",
      "|2018-01-01 01:00:00|5.793007851|\n",
      "|2018-01-01 01:10:00|5.306049824|\n",
      "|2018-01-01 01:20:00|5.584629059|\n",
      "|2018-01-01 01:30:00|5.523228168|\n",
      "|2018-01-01 01:40:00|5.724115849|\n",
      "|2018-01-01 01:50:00|5.934198856|\n",
      "|2018-01-01 02:00:00|6.547413826|\n",
      "|2018-01-01 02:10:00|6.199746132|\n",
      "|2018-01-01 02:20:00|6.505383015|\n",
      "|2018-01-01 02:30:00|6.634116173|\n",
      "|2018-01-01 02:40:00|6.378912926|\n",
      "|2018-01-01 02:50:00|6.446652889|\n",
      "|2018-01-01 03:00:00|6.415082932|\n",
      "|2018-01-01 03:10:00|6.437530994|\n",
      "+-------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_timestamp\n",
    "\n",
    "\n",
    "input_time_format = \"dd MM yyyy HH:mm\"\n",
    "output_time_format = \"yyyy MM dd HH:mm\"\n",
    "\n",
    "# Convert the timestamp format\n",
    "dataset = dataset.withColumn(\"timeStamp\", to_timestamp(col(\"timeStamp\"), input_time_format))\n",
    "\n",
    "# Format the timestamp to the desired output format\n",
    "dataset = dataset.withColumn(\"timeStamp\", dataset[\"timeStamp\"].cast(\"string\"))\n",
    "dataset.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql.functions import col\n",
    "\n",
    "# Convert timeStamp column to timestamp type\n",
    "dataset = dataset.withColumn(\"timeStamp\", col(\"timeStamp\").cast(\"timestamp\"))\n",
    "\n",
    "# Set timeStamp as the index\n",
    "dataset = dataset.withColumn(\"index\", col(\"timeStamp\").cast(\"long\")).sort(\"index\").drop(\"index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Previous code...)\n",
    "\n",
    "# Impute missing values using the lag function\n",
    "dataset = dataset.withColumn(\"windSpeed_imputed\", when(col(\"windSpeed\").isNull(), lag(\"windSpeed\").over(windowSpec)).otherwise(col(\"windSpeed\")))\n",
    "\n",
    "# Fill initial null values with a default value\n",
    "dataset = dataset.na.fill(0, subset=[\"windSpeed_imputed\"])\n",
    "\n",
    "# (Continued code...)\n",
    "\n",
    "# Create lag features\n",
    "for i in range(1, 5):\n",
    "    dataset = dataset.withColumn(f\"lag_{i}\", lag(\"windSpeed_imputed\", i).over(windowSpec))\n",
    "\n",
    "# Drop rows with missing lag features\n",
    "dataset = dataset.dropna()\n",
    "\n",
    "# (Continued code...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a column for ticks\n",
    "dataset = dataset.withColumn(\"Ticks\", (lag(\"windSpeed\").over(windowSpec)).isNull().cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot original data\n",
    "original_data = dataset.toPandas()\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(original_data[\"Ticks\"], original_data[\"windSpeed_imputed\"])\n",
    "plt.xlabel(\"Ticks\")\n",
    "plt.ylabel(\"Wind Speed (m/s)\")\n",
    "plt.title(\"Original Plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for stationarity check using ADF test\n",
    "def stationarity_check(df):\n",
    "    df_values = df.select(\"windSpeed_imputed\").toPandas()\n",
    "    result = adfuller(df_values[\"windSpeed_imputed\"])\n",
    "    print('Augmented Dickey-Fuller test:')\n",
    "    print(f'Test Statistic: {result[0]}')\n",
    "    print(f'p-value: {result[1]}')\n",
    "    print(f'Critical Values:')\n",
    "    for key, value in result[4].items():\n",
    "        print(f'   {key}: {value}')\n",
    "\n",
    "# Apply stationarity check\n",
    "stationarity_check(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lag features\n",
    "for i in range(1, 5):\n",
    "    dataset = dataset.withColumn(f\"lag_{i}\", lag(\"windSpeed_imputed\", i).over(windowSpec))\n",
    "\n",
    "# Drop rows with missing lag features\n",
    "dataset = dataset.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble features\n",
    "feature_cols = [\"lag_1\", \"lag_2\", \"lag_3\", \"lag_4\"]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "dataset = assembler.transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit ARMA model\n",
    "arma = ARMA(spark, labelCol=\"windSpeed_imputed\", featuresCol=\"features\", p=3, q=3)\n",
    "model = arma.fit(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions\n",
    "predictions = model.transform(dataset)\n",
    "predictions_pd = predictions.select(\"timeStamp\", \"prediction\").toPandas()\n",
    "plt.plot(original_data[\"Ticks\"], original_data[\"windSpeed_imputed\"], label=\"Actual\")\n",
    "plt.plot(predictions_pd[\"timeStamp\"], predictions_pd[\"prediction\"], label=\"Predicted\")\n",
    "plt.xlabel(\"Ticks\")\n",
    "plt.ylabel(\"Wind Speed (m/s)\")\n",
    "plt.title(\"Fitted data\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save(\"humidityModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate future dates\n",
    "future_dates = [pd.Timestamp(original_data[\"timeStamp\"].iloc[-1]) + pd.DateOffset(months=x) for x in range(1, 25)]\n",
    "future_datest_df = pd.DataFrame(index=future_dates[1:], columns=original_data.columns)\n",
    "future_df = pd.concat([original_data, future_datest_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for future dates\n",
    "future_df[\"forecast\"] = model.transform(assembler.transform(spark.createDataFrame(future_df))).select(\"prediction\").toPandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
