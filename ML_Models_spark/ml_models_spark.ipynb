{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.regression import RandomForestRegressor, LinearRegression, DecisionTreeRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidatorModel\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.feature import VectorAssembler, MinMaxScaler\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SparkSession\n",
    "os.environ['SPARK_HOME'] = 'C:/spark-3.5.0-bin-hadoop3'\n",
    "os.environ['PATH'] += 'C:/spark-3.5.0-bin-hadoop3/bin'\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Wind Power Prediction\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = spark.read.csv('../Dataset/T1.csv', header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove irrelevant columns\n",
    "df = df.drop('Date/Time', 'Theoretical_Power_Curve (KWh)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[LV ActivePower (kW): double, Wind Speed (m/s): double, Wind Direction (°): double]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features\n",
    "feature_columns = ['Wind Direction (°)', 'Wind Speed (m/s)']\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol='features')\n",
    "scaler = MinMaxScaler(inputCol='features', outputCol='scaled_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    'XGBoost': GBTRegressor(labelCol='LV ActivePower (kW)'),\n",
    "    'Random Forest': RandomForestRegressor(labelCol='LV ActivePower (kW)'),\n",
    "    'Linear Regression': LinearRegression(labelCol='LV ActivePower (kW)'),\n",
    "    'Decision Tree': DecisionTreeRegressor(labelCol='LV ActivePower (kW)')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluator\n",
    "evaluator = RegressionEvaluator(labelCol=\"LV ActivePower (kW)\", predictionCol=\"prediction\", metricName=\"r2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2-XGBoost: 0.9163568488646847\n",
      "R2-Random Forest: 0.9087736937295984\n",
      "R2-Linear Regression: 0.8335548083227226\n",
      "R2-Decision Tree: 0.9137755646017032\n"
     ]
    }
   ],
   "source": [
    "for name, model in models.items():\n",
    "    pipeline = Pipeline(stages=[assembler, scaler, model])\n",
    "    paramGrid = ParamGridBuilder().build()\n",
    "    crossval = CrossValidator(estimator=pipeline,\n",
    "                              estimatorParamMaps=paramGrid,\n",
    "                              evaluator=evaluator,\n",
    "                              numFolds=5)\n",
    "    cvModel = crossval.fit(train_data)\n",
    "    predictions = cvModel.transform(test_data)\n",
    "    r2 = evaluator.evaluate(predictions)\n",
    "    print(f'R2-{name}: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting findspark\n",
      "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
      "Using cached findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-2.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\python311\\lib\\site-packages\\vboxapi-1.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n"
     ]
    }
   ],
   "source": [
    "! pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2-XGBoost: 0.9179209624304278\n",
      "R2-Random Forest: 0.9063155313901088\n",
      "R2-Linear Regression: 0.8335548083227226\n",
      "R2-Decision Tree: 0.9137816673439929\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import MinMaxScaler, VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor, LinearRegression, DecisionTreeRegressor, GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "import findspark\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Specify the Spark home directory and version\n",
    "findspark.init('C:/spark/spark-3.5.1-bin-hadoop3/spark-3.5.1-bin-hadoop3')\n",
    "\n",
    "# Configure Spark to use a master URL and set up the application name\n",
    "master_url = \"spark://192.168.57.215:7077\"\n",
    "app_name = \"spark-basic\"\n",
    "\n",
    "# Create a SparkSession with the specified master and app name\n",
    "spark = SparkSession.builder.master(master_url).appName(app_name).getOrCreate()\n",
    "\n",
    "# Load data from a local CSV file\n",
    "df = spark.read.csv('../Dataset/T1.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Remove irrelevant columns\n",
    "df = df.drop('Date/Time', 'Theoretical_Power_Curve (KWh)')\n",
    "\n",
    "# Define feature and target columns\n",
    "feature_columns = ['Wind Direction (°)', 'Wind Speed (m/s)']\n",
    "target_col = 'LV ActivePower (kW)'\n",
    "\n",
    "# Normalize features\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol='features')\n",
    "scaler = MinMaxScaler(inputCol='features', outputCol='scaled_features')\n",
    "\n",
    "# Define models dictionary\n",
    "models = {\n",
    "    'XGBoost': GBTRegressor(labelCol=target_col),\n",
    "    'Random Forest': RandomForestRegressor(labelCol=target_col),\n",
    "    'Linear Regression': LinearRegression(labelCol=target_col),\n",
    "    'Decision Tree': DecisionTreeRegressor(labelCol=target_col)\n",
    "}\n",
    "\n",
    "# Define evaluator\n",
    "evaluator = RegressionEvaluator(labelCol=target_col, predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Parallel processing for model training\n",
    "def fit_pipeline(model, train_data):\n",
    "    pipeline = Pipeline(stages=[assembler, scaler, model])\n",
    "    return pipeline.fit(train_data)\n",
    "\n",
    "for name, model in models.items():\n",
    "    pipeline_model = fit_pipeline(model, train_data)\n",
    "    predictions = pipeline_model.transform(test_data)\n",
    "    r2 = evaluator.evaluate(predictions)\n",
    "    print(f'R2-{name}: {r2}')\n",
    "\n",
    "   \n",
    "\n",
    "# Stop SparkSession\n",
    "spark.stop()\n",
    "# Define evaluator for RMSE\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
